# Sign Language Recognition with Machine Learning

This project develops a sign language recognition system using machine learning techniques to assist communication for the hearing impaired.

## Problem Statement

Sign language is crucial for communication among the hearing impaired but is not widely understood. This project aims to create a system that translates sign language gestures into written or spoken language, enhancing communication and improving quality of life.

## Objectives

- **Data Collection and Preprocessing:** Gather and preprocess sign language data from images and videos.
- **Feature Extraction and Selection:** Identify key features that represent sign language gestures.
- **Model Training:** Train machine learning models using KNN, CNN, Linear Regression, and Random Forest.
- **Model Evaluation and Adjustment:** Evaluate and fine-tune model performance.
- **Real-Time Application:** Develop a real-time application to recognize and translate gestures.

## Dataset

- **American Sign Language Dataset:** Includes images of hand gestures representing different letters of the alphabet.
- **Source:** [Sign Language MNIST on Kaggle](https://www.kaggle.com/datasets/datamunge/sign-language-mnist)

## Getting Started

1. Clone the repository:
    ```bash
    git clone [Repository URL]
    ```
2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3. Run the main script:
    ```bash
    python main.py
    ```

## Files

- `main.py`: Script for training and evaluating the model.
- `requirements.txt`: Required Python libraries.
- `README.md`: Documentation for the project.

## License

MIT License
